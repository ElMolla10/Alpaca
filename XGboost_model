{"ells":[{"cell_type":"code","source":["import os\n","\n","# Paste your Polygon key once per session\n","POLYGON_API_KEY = \"fbrZKMMbLhV7p0Rz_C9PKM69eps7P8OG\"\n","os.environ[\"fbrZKMMbLhV7p0Rz_C9PKM69eps7P8OG\"] = POLYGON_API_KEY\n"],"metadata":{"id":"9znSaO8_0wPG","executionInfo":{"status":"ok","timestamp":1759938897422,"user_tz":-180,"elapsed":13,"user":{"displayName":"Mohamed Ehab","userId":"16063572679742729614"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2d007f7e","executionInfo":{"status":"ok","timestamp":1759938914109,"user_tz":-180,"elapsed":16673,"user":{"displayName":"Mohamed Ehab","userId":"16063572679742729614"}},"outputId":"435bf7b2-ccc0-44a8-d69f-2635e4af2837"},"source":["%pip install pandas_ta"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas_ta in /usr/local/lib/python3.12/dist-packages (0.4.71b0)\n","Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.12/dist-packages (from pandas_ta) (0.61.2)\n","Requirement already satisfied: numpy>=2.2.6 in /usr/local/lib/python3.12/dist-packages (from pandas_ta) (2.2.6)\n","Requirement already satisfied: pandas>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from pandas_ta) (2.3.3)\n","Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from pandas_ta) (4.67.1)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba==0.61.2->pandas_ta) (0.44.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.3.2->pandas_ta) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.3.2->pandas_ta) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.3.2->pandas_ta) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.3.2->pandas_ta) (1.17.0)\n"]}]},{"cell_type":"code","source":["# ===========================\n","# FULL UPDATED CELL — Multi-Ticker Training & Backtest (drop-in)\n","# ===========================\n","import os, re, glob, math\n","import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","from sklearn.model_selection import TimeSeriesSplit\n","\n","# ===========================\n","# CONFIG\n","# ===========================\n","INITIAL_CAPITAL = 1000\n","LEVERAGE = 3.0                   # notional at |pos|=1\n","STOP_LOSS_PCT = 2.0              # per-block max loss on leveraged path, %\n","HORIZON_LIST = [1, 3, 6]         # horizons to stack (hours)\n","PRIMARY_H = 1\n","TRADE_COST_BPS = 8.0             # per unit of |Δposition| (enter/resize/exit)\n","SLIPPAGE_BPS_PER_TRADE = 4.0\n","BORROW_BPS_PER_HOUR = 0.5        # shorts funding per hour\n","LONG_FUND_BPS_PER_HOUR = 0.1     # longs funding per hour\n","N_SPLITS = 5\n","NUM_BOOST_ROUND = 1200\n","EARLY_STOP = 100\n","\n","TRADING_HOURS_PER_DAY = 6\n","BLOCKS_PER_DAY = TRADING_HOURS_PER_DAY / PRIMARY_H\n","\n","# Friday behavioral modifiers for backtest parity with live trading\n","FRIDAY_SIZE_MULT_DAY = 0.75        # size×=0.75 before 14:30 ET\n","FRIDAY_SIZE_MULT_LATE = 0.50       # size×=0.50 after 14:30 ET\n","FRIDAY_LATE_CUTOFF_H = 14          # hour threshold for \"late\" (2pm ET)\n","FRIDAY_BLOCK_NEW_AFTER_LATE = True # block NEW entries after cutoff\n","FRIDAY_MIN_POS = 0.03              # skip tiny trades under 3%\n","\n","# Safety clamps\n","CLIP_BLOCK_RET_PCT = 1.0         # clamp per-block net return to ±1%\n","CLIP_HOURLY_RET = 8.0            # clamp raw hourly return to ±8%\n","\n","# Continuous sizing grid: full leverage if |pred_return_pct| >= band_R\n","BAND_R_GRID = np.linspace(0.8, 2.0, 13)   # % per 3h block\n","HALF_LIFE_GRID = [4, 6, 8, 12]            # smoother set\n","TURNOVER_PENALTY = 1.00                   # penalize ToV harder\n","DPOS_CAP = 0.10\n","\n","# --- Tuned XGB params ---\n","best_params_hardcoded = {\n","    'learning_rate': 0.015606457197928938,\n","    'max_depth': 6,\n","    'min_child_weight': 3,\n","    'subsample': 0.745376365593323,\n","    'colsample_bytree': 0.8951313195304963,\n","    'reg_lambda': 0.9504223823141223,\n","    'reg_alpha': 1.2822373353472882,\n","    'gamma': 3.890792160899413\n","}\n","xgb_reg_params = dict(\n","    objective=\"reg:squarederror\",\n","    tree_method=\"hist\",\n","    eval_metric=\"rmse\",\n","    seed=42,\n","    nthread=-1,\n",")\n","xgb_reg_params.update(best_params_hardcoded)\n","\n","# ===========================\n","# Metrics\n","# ===========================\n","def safe_sharpe_annualized(returns_pct):\n","    r = np.asarray(returns_pct, float) / 100.0\n","    if r.size < 2:\n","        return 0.0\n","    sd = r.std()\n","    if sd == 0 or not np.isfinite(sd):\n","        return 0.0\n","    return (r.mean() / sd) * math.sqrt(BLOCKS_PER_DAY * 252)\n","\n","def compute_cagr(returns_pct):\n","    r = np.asarray(returns_pct, float)\n","    if r.size == 0:\n","        return 0.0\n","    growth = np.prod(1.0 + r / 100.0)\n","    years = r.size / (BLOCKS_PER_DAY * 252.0)\n","    return 0.0 if years <= 0 else (growth ** (1.0 / years) - 1.0) * 100.0\n","\n","# ===========================\n","# Continuous-position backtest (non-overlapping PRIMARY_H-hour blocks)\n","# ===========================\n","def backtest_continuous_by_return(\n","    pred_block_ret_pct, hourly_ret_pct,\n","    horizon_h=3, leverage=3.0, stop_loss_pct=2.0,\n","    trade_cost_bps=8.0, slippage_bps=4.0,\n","    borrow_bps_per_hour=0.5, long_fund_bps_per_hour=0.1,\n","    band_R=1.10,            # % at which |pos_raw|=1 (use CV result as default)\n","    ema_half_life=8,        # stronger smoothing\n","    dpos_cap=0.10,          # smaller re-hedge step\n","    hold_min_blocks=2,      # stick with a position at least N blocks\n","    target_vol_pct=0.30,    # target 3h *net* return vol per block (%)\n","    vol_lookback_blocks=40, # forecast vol lookback (blocks)\n","    clip_block_ret_pct=1.0, clip_hourly_ret=8.0,\n","    return_series=False\n","):\n","    pred = np.asarray(pred_block_ret_pct, float)\n","    r_hour = np.asarray(hourly_ret_pct, float)\n","\n","    n = min(len(pred), len(r_hour))\n","    if n < horizon_h:\n","        out = {\"sharpe\": 0.0, \"avg_turnover\": 0.0}\n","        if return_series: out[\"net_returns\"] = np.array([]); out[\"positions\"] = np.array([])\n","        return out\n","\n","    # Block partition\n","    idx = np.arange(0, n - horizon_h + 1, horizon_h)\n","    B = len(idx)\n","    if B == 0:\n","        out = {\"sharpe\": 0.0, \"avg_turnover\": 0.0}\n","        if return_series: out[\"net_returns\"] = np.array([]); out[\"positions\"] = np.array([])\n","        return out\n","\n","    # Raw position from value signal\n","    s = pred[idx] / max(1e-12, band_R)\n","    s = np.clip(s, -10.0, 10.0)\n","\n","    # Forecast block volatility via rolling std of realized block returns\n","    block_unlev = np.stack([np.clip(r_hour[idx + k], -clip_hourly_ret, clip_hourly_ret)\n","                            for k in range(horizon_h)], axis=1).sum(axis=1)\n","    fvol = np.full(B, np.nan, float)\n","    for t in range(B):\n","        j0 = max(0, t - vol_lookback_blocks)\n","        if t - j0 >= 10:\n","            fvol[t] = block_unlev[j0:t].std()\n","    fvol = np.nan_to_num(fvol, nan=np.nanmedian(block_unlev[: max(10, B//5)].std() if B>10 else 1.0))\n","    fvol = np.where(fvol <= 1e-6, 1e-6, fvol)\n","\n","    # Risk targeting scale\n","    k = target_vol_pct / fvol\n","    k = np.clip(k, 0.2, 5.0)\n","\n","    pos_raw = np.clip(s * k, -1.0, 1.0)\n","\n","        # --- Friday scaling simulation ---\n","    # Determine if each block falls on a Friday and whether it's \"late\"\n","    # For simplicity, assume 6 trading hours per day starting at 10:00 ET.\n","    hours = np.arange(B) * horizon_h\n","    day_index = (hours // TRADING_HOURS_PER_DAY).astype(int)\n","    hour_of_day = (hours % TRADING_HOURS_PER_DAY) + 10  # starts at 10 ET\n","\n","    # Apply Friday rules\n","    friday_blocks = (day_index % 5 == 4)  # every 5th day = Friday (0=Mon)\n","    late_blocks = (hour_of_day >= FRIDAY_LATE_CUTOFF_H)\n","\n","    friday_mult = np.ones(B)\n","    friday_mult[np.logical_and(friday_blocks, ~late_blocks)] = FRIDAY_SIZE_MULT_DAY\n","    friday_mult[np.logical_and(friday_blocks, late_blocks)] = FRIDAY_SIZE_MULT_LATE\n","\n","    # Scale positions for Friday size reduction\n","    pos_raw *= friday_mult\n","\n","    # Optionally zero out new entries after late Friday cutoff\n","    if FRIDAY_BLOCK_NEW_AFTER_LATE:\n","        for t in range(1, B):\n","            if friday_blocks[t] and late_blocks[t]:\n","                if np.sign(pos_raw[t]) != np.sign(pos_raw[t-1]):\n","                    pos_raw[t] = 0.0\n","\n","\n","    # EMA smoothing\n","    if ema_half_life <= 1:\n","        pos_smooth = pos_raw.copy()\n","    else:\n","        alpha = 1.0 - 0.5 ** (1.0 / float(ema_half_life))\n","        pos_smooth = np.empty_like(pos_raw)\n","        pos_smooth[0] = pos_raw[0]\n","        for t in range(1, B):\n","            pos_smooth[t] = alpha * pos_raw[t] + (1.0 - alpha) * pos_smooth[t-1]\n","\n","    # Re-hedge cap & min-hold\n","    pos = pos_smooth.copy()\n","    for t in range(1, B):\n","        delta = pos[t] - pos[t-1]\n","        if delta >  dpos_cap: pos[t] = pos[t-1] + dpos_cap\n","        if delta < -dpos_cap: pos[t] = pos[t-1] - dpos_cap\n","\n","    if hold_min_blocks > 1 and B > 1:\n","        last_change = 0\n","        for t in range(1, B):\n","            if np.sign(pos[t]) != np.sign(pos[t-1]) and t - last_change < hold_min_blocks:\n","                pos[t] = pos[t-1]\n","            else:\n","                if np.sign(pos[t]) != np.sign(pos[t-1]): last_change = t\n","\n","    # P&L path inside each block with stop\n","    block_rets = np.zeros(B)\n","    hours_held = np.zeros(B, dtype=int)\n","    for i, start in enumerate(idx):\n","        p = pos[i]\n","        if p == 0.0:\n","            continue\n","        cum = 0.0; held = 0\n","        for h in range(horizon_h):\n","            rh = float(np.clip(r_hour[start + h], -clip_hourly_ret, clip_hourly_ret))\n","            cum += np.sign(p) * abs(p) * (rh * LEVERAGE)\n","            held += 1\n","            if cum <= -STOP_LOSS_PCT:\n","                cum = -STOP_LOSS_PCT\n","                break\n","        block_rets[i] = cum\n","        hours_held[i] = held\n","\n","    # Costs on |Δpos|\n","    dpos = np.r_[pos[0], np.diff(pos)]\n","    trade_cost_pct = (np.abs(dpos) * (TRADE_COST_BPS + SLIPPAGE_BPS_PER_TRADE) / 100.0) * LEVERAGE\n","\n","    # Funding\n","    long_mask  = (pos > 0).astype(float)\n","    short_mask = (pos < 0).astype(float)\n","    long_fund_pct  = long_mask  * (LONG_FUND_BPS_PER_HOUR / 100.0) * hours_held * LEVERAGE * np.abs(pos)\n","    borrow_pct     = short_mask * (BORROW_BPS_PER_HOUR    / 100.0) * hours_held * LEVERAGE * np.abs(pos)\n","\n","    net = block_rets - trade_cost_pct - long_fund_pct - borrow_pct\n","    if clip_block_ret_pct is not None:\n","        net = np.clip(net, -abs(clip_block_ret_pct), abs(clip_block_ret_pct))\n","\n","    def _safe_sharpe(returns_pct):\n","        r = np.asarray(returns_pct, float) / 100.0\n","        if r.size < 2: return 0.0\n","        sd = r.std()\n","        if sd == 0 or not np.isfinite(sd): return 0.0\n","        return (r.mean() / sd) * math.sqrt((6/horizon_h) * 252)\n","\n","    turnover = float(np.mean(np.abs(dpos) > 1e-6)) if B > 1 else 0.0\n","    res = {\"sharpe\": float(_safe_sharpe(net)), \"avg_turnover\": turnover}\n","    if return_series:\n","        res[\"net_returns\"] = net\n","        res[\"positions\"] = pos\n","    return res\n","\n","# ===========================\n","# Feature builder with volatility normalization\n","# ===========================\n","def build_features_with_volnorm(df, horizons):\n","    \"\"\"\n","    Returns:\n","      df_filt              : filtered df (regular hours)\n","      feat_cols            : feature names\n","      X_dict[h]            : features matrix for horizon h\n","      y_reg_norm_dict[h]   : normalized regression targets for horizon h\n","      sigma20_pct          : rolling sigma_20 (percent of 1h returns), lagged\n","      hourly_ret_pct_all   : unlevered 1h path in percent (for backtest)\n","    \"\"\"\n","    df = df.copy()\n","    if 'timestamp' not in df.columns:\n","        df.reset_index(inplace=True)\n","        if 'date' in df.columns and 'timestamp' not in df.columns:\n","            df.rename(columns={'date': 'timestamp'}, inplace=True)\n","        elif 'time' in df.columns and 'timestamp' not in df.columns:\n","            df.rename(columns={'time': 'timestamp'}, inplace=True)\n","\n","    ts = pd.to_datetime(df[\"timestamp\"], utc=True)\n","    hours_et = ts.dt.tz_convert(\"America/New_York\").dt.hour\n","    keep = hours_et.between(10, 15)   # 10:00–15:59 ET\n","    df = df.loc[keep].reset_index(drop=True)\n","\n","    # 1h returns (percent)\n","    df[\"ret1h_pct\"] = df[\"close\"].pct_change().shift(-1) * 100.0\n","    # Rolling sigma of 1h returns in %, lagged to avoid lookahead\n","    df[\"sigma20_pct\"] = df[\"ret1h_pct\"].rolling(20).std().shift(1)\n","    df[\"sigma20_pct\"] = df[\"sigma20_pct\"].replace(0.0, np.nan)\n","\n","    # Base features (create if missing)\n","    base = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"vwap\",\n","            \"price_change_pct\",\"MACDh_12_26_9\",\"BBM_20_2.0\"]\n","    for f in base:\n","        if f not in df.columns:\n","            if f == 'price_change_pct':\n","                df[f] = df['close'].pct_change() * 100.0\n","            else:\n","                df[f] = 0.0\n","\n","    # Lag one to remove look-ahead\n","    for f in base:\n","        df[f] = df[f].shift(1)\n","\n","    # Extra lags\n","    for f in [\"price_change_pct\",\"MACDh_12_26_9\",\"BBM_20_2.0\"]:\n","        df[f+\"_lag2\"] = df[f].shift(2)\n","        df[f+\"_lag3\"] = df[f].shift(3)\n","\n","    # Additional features\n","    df[\"vol20\"] = df[\"ret1h_pct\"].rolling(20).std().shift(1)\n","    df[\"ret5\"]  = df[\"close\"].pct_change(5).shift(1) * 100.0\n","\n","    # Normalize selected % features by sigma20 to reduce regime sensitivity\n","    for f in [\"price_change_pct\",\"ret5\",\"vol20\",\"MACDh_12_26_9\",\"BBM_20_2.0\"]:\n","        if f in df.columns:\n","            df[f+\"_N\"] = df[f] / df[\"sigma20_pct\"]\n","\n","    # Build regression targets per horizon (percent) and normalize by sigma20\n","    y_reg_norm_dict = {}\n","    for H in horizons:\n","        df[f\"y_reg_{H}h_pct\"] = df[\"close\"].pct_change(H).shift(-H) * 100.0\n","        y_reg_norm_dict[H] = (df[f\"y_reg_{H}h_pct\"] / df[\"sigma20_pct\"])\n","\n","    # Hourly path for backtest (clip to kill errors)\n","    hourly_ret_pct_all = df[\"ret1h_pct\"].fillna(0.0).clip(lower=-CLIP_HOURLY_RET, upper=CLIP_HOURLY_RET)\n","\n","    # Drop NaNs from all engineered columns\n","    df = df.dropna().reset_index(drop=True)\n","\n","    # Features to use (original + normalized)\n","    use_cols = []\n","    for f in base + [\"price_change_pct_lag2\",\"price_change_pct_lag3\",\n","                     \"MACDh_12_26_9_lag2\",\"MACDh_12_26_9_lag3\",\n","                     \"BBM_20_2.0_lag2\",\"BBM_20_2.0_lag3\",\n","                     \"vol20\",\"ret5\",\n","                     \"price_change_pct_N\",\"ret5_N\",\"vol20_N\",\"MACDh_12_26_9_N\",\"BBM_20_2.0_N\"]:\n","        if f in df.columns:\n","            use_cols.append(f)\n","    feat_cols = use_cols\n","\n","    # Feature matrices & normalized targets aligned to filtered df\n","    X = df[feat_cols].astype(float).values\n","    sigma20_pct = df[\"sigma20_pct\"].to_numpy()\n","    X_dict = {H: X for H in horizons}\n","    y_norm = {}\n","    for H in horizons:\n","        y_norm[H] = df[f\"y_reg_{H}h_pct\"].to_numpy() / sigma20_pct\n","    hourly_ret_pct_all = hourly_ret_pct_all.iloc[:len(df)].to_numpy()\n","\n","    return df, feat_cols, X_dict, y_norm, sigma20_pct, hourly_ret_pct_all\n","\n","# ===========================\n","# Tune band_R on CV to maximize Sharpe\n","# ===========================\n","def joint_tune(pred, hourly_ret, horizon_h):\n","    best = {\"score\": -1e9, \"sign\": 1, \"band_R\": None, \"hl\": None, \"report\": None}\n","    for sgn in (1, -1):\n","        pred_s = sgn * pred\n","        for band in BAND_R_GRID:\n","            for hl in HALF_LIFE_GRID:\n","                m = backtest_continuous_by_return(\n","                    pred_block_ret_pct=pred_s,\n","                    hourly_ret_pct=hourly_ret,\n","                    horizon_h=horizon_h,\n","                    band_R=band,\n","                    ema_half_life=hl,\n","                    dpos_cap=DPOS_CAP,\n","                    return_series=False\n","                )\n","                score = m[\"sharpe\"] - TURNOVER_PENALTY * m[\"avg_turnover\"]\n","                if score > best[\"score\"]:\n","                    best = {\"score\": score, \"sign\": sgn, \"band_R\": float(band), \"hl\": int(hl), \"report\": m}\n","    return best\n","\n","# ===========================\n","# Investment summary helpers\n","# ===========================\n","def simulate_investment(returns_pct_series, initial_capital):\n","    r = np.asarray(returns_pct_series, float)\n","    if r.size == 0:\n","        return {\"total_return_pct\": 0.0, \"final_equity\": initial_capital,\n","                \"profit\": 0.0, \"max_drawdown_pct\": 0.0, \"cagr_pct\": 0.0}\n","    r = np.nan_to_num(r, nan=0.0, posinf=0.0, neginf=0.0)\n","    r = np.clip(r, -50.0, 50.0)\n","    eq = np.empty(r.size + 1); eq[0] = initial_capital\n","    for i in range(r.size): eq[i+1] = eq[i] * (1.0 + r[i] / 100.0)\n","    final_equity = float(eq[-1])\n","    total_return_pct = (final_equity / initial_capital - 1.0) * 100.0\n","    profit = final_equity - initial_capital\n","    peaks = np.maximum.accumulate(eq)\n","    dd = (eq - peaks) / peaks\n","    max_drawdown_pct = float(dd.min() * 100.0)\n","    years = r.size / ((6/PRIMARY_H) * 252.0)\n","    cagr_pct = 0.0 if years <= 0 else ((final_equity / initial_capital) ** (1/years) - 1) * 100.0\n","    return {\"total_return_pct\": float(total_return_pct),\n","            \"final_equity\": final_equity,\n","            \"profit\": float(profit),\n","            \"max_drawdown_pct\": max_drawdown_pct,\n","            \"cagr_pct\": float(cagr_pct)}\n","\n","# ===========================\n","# Multi-Ticker Runner\n","# ===========================\n","def _run_one_ticker(path_csv):\n","    # 1) read & symbol\n","    df_base = pd.read_csv(path_csv)\n","    m = re.search(r'/([A-Z]+)_hourly_last5y_selected_features\\.csv', path_csv)\n","    sym = m.group(1) if m else os.path.basename(path_csv).split('_')[0]\n","\n","    # 2) build features\n","    (df_feat, feat_cols,\n","     X_dict, y_reg_norm_dict,\n","     sigma20_pct, hourly_ret_pct_all) = build_features_with_volnorm(df_base, HORIZON_LIST)\n","\n","    # 3) OOF training per horizon\n","    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n","    n = len(df_feat)\n","    oof_pred_norm = {H: np.full(n, np.nan, float) for H in HORIZON_LIST}\n","\n","    for H in HORIZON_LIST:\n","        X = X_dict[H]\n","        y = y_reg_norm_dict[H]\n","        for fold, (tr_idx, te_idx) in enumerate(tscv.split(X), 1):\n","            tr = tr_idx[:-H] if len(tr_idx) > H else tr_idx  # embargo\n","            dtr = xgb.DMatrix(X[tr], label=y[tr])\n","            dte = xgb.DMatrix(X[te_idx], label=y[te_idx])\n","            bst = xgb.train(\n","                params=xgb_reg_params,\n","                dtrain=dtr,\n","                num_boost_round=NUM_BOOST_ROUND,\n","                evals=[(dtr,\"train\"), (dte,\"eval\")],\n","                early_stopping_rounds=EARLY_STOP,\n","                verbose_eval=False\n","            )\n","            oof_pred_norm[H][te_idx] = bst.predict(dte)\n","\n","    # 4) stack normalized predictions → percent per PRIMARY_H block\n","    stack_pred_norm = np.zeros(n); count = np.zeros(n)\n","    for H in HORIZON_LIST:\n","        mask = ~np.isnan(oof_pred_norm[H])\n","        stack_pred_norm[mask] += oof_pred_norm[H][mask]; count[mask] += 1\n","    valid_mask = count > 0\n","    stack_pred_norm[valid_mask] /= count[valid_mask]\n","\n","    pred_block_ret_pct = stack_pred_norm * sigma20_pct\n","    pred_block_ret_pct = np.nan_to_num(pred_block_ret_pct, nan=0.0, posinf=0.0, neginf=0.0)\n","    pred_block_ret_pct_scaled = pred_block_ret_pct * math.sqrt(PRIMARY_H)\n","\n","    # 5) tune policy\n","    cv_best = joint_tune(pred_block_ret_pct_scaled, hourly_ret_pct_all, PRIMARY_H)\n","\n","    # 6) align & holdout mask\n","    def _align_to_common_length(*objs):\n","        nmin = min(len(o) for o in objs)\n","        out = []\n","        for o in objs:\n","            if hasattr(o, \"iloc\"): out.append(o.iloc[:nmin])\n","            else: out.append(o[:nmin])\n","        return out, nmin\n","\n","    (pred_block_ret_pct_scaled,\n","     hourly_ret_pct_all,\n","     df_feat), n_common = _align_to_common_length(\n","        pred_block_ret_pct_scaled, hourly_ret_pct_all, df_feat\n","    )\n","    split = int(n_common * 0.8)\n","    mask_hold = np.zeros(n_common, dtype=bool); mask_hold[split:] = True\n","\n","    print(f\"[SIM] Applying Friday scaling: day×={FRIDAY_SIZE_MULT_DAY}, late×={FRIDAY_SIZE_MULT_LATE}, block_new_after_late={FRIDAY_BLOCK_NEW_AFTER_LATE}\")\n","\n","\n","    # 7) final CV & Holdout backtests\n","    m_all = backtest_continuous_by_return(\n","        pred_block_ret_pct=cv_best[\"sign\"] * pred_block_ret_pct_scaled,\n","        hourly_ret_pct=hourly_ret_pct_all,\n","        horizon_h=PRIMARY_H,\n","        band_R=cv_best[\"band_R\"],\n","        ema_half_life=cv_best[\"hl\"],\n","        dpos_cap=DPOS_CAP,\n","        return_series=True\n","    )\n","    m_hold = backtest_continuous_by_return(\n","        pred_block_ret_pct=(cv_best[\"sign\"] * pred_block_ret_pct_scaled)[mask_hold],\n","        hourly_ret_pct=hourly_ret_pct_all[mask_hold],\n","        horizon_h=PRIMARY_H,\n","        band_R=cv_best[\"band_R\"],\n","        ema_half_life=cv_best[\"hl\"],\n","        dpos_cap=DPOS_CAP,\n","        return_series=True\n","    )\n","\n","    # 8) investment stats\n","    cv_results = simulate_investment(m_all['net_returns'], INITIAL_CAPITAL)\n","    holdout_start = len(m_all['net_returns']) - len(m_hold['net_returns'])\n","    cap_at_hold = simulate_investment(m_all['net_returns'][:holdout_start], INITIAL_CAPITAL)['final_equity']\n","    holdout_results = simulate_investment(m_hold['net_returns'], cap_at_hold)\n","\n","    # 9) summary row\n","    row = {\n","        \"symbol\": sym,\n","        \"n_blocks\": int(len(m_all['net_returns'])),\n","        \"cv_sharpe\": float(m_all['sharpe']),\n","        \"cv_turnover\": float(m_all['avg_turnover']),\n","        \"cv_total_return_pct\": float(cv_results['total_return_pct']),\n","        \"cv_cagr_pct\": float(cv_results['cagr_pct']),\n","        \"cv_max_dd_pct\": float(cv_results['max_drawdown_pct']),\n","        \"hold_sharpe\": float(m_hold['sharpe']),\n","        \"hold_turnover\": float(m_hold['avg_turnover']),\n","        \"hold_total_return_pct\": float(holdout_results['total_return_pct']),\n","        \"hold_cagr_pct\": float(holdout_results['cagr_pct']),\n","        \"hold_max_dd_pct\": float(holdout_results['max_drawdown_pct']),\n","        \"band_R\": float(cv_best['band_R']),\n","        \"ema_half_life\": int(cv_best['hl']),\n","        \"sign\": int(cv_best['sign'])\n","    }\n","    return row\n","\n","# ===========================\n","# Discover files, run all, summarize\n","# ===========================\n","DATA_DIR = \"/content\"  # change if needed\n","files = sorted(glob.glob(os.path.join(DATA_DIR, \"*_hourly_last5y_selected_features.csv\")))\n","if not files:\n","    raise FileNotFoundError(f\"No matching CSVs found in {DATA_DIR}\")\n","\n","summary = []\n","for i, fpath in enumerate(files, 1):\n","    print(f\"\\n[{i}/{len(files)}] Running: {fpath}\")\n","    try:\n","        row = _run_one_ticker(fpath)\n","        summary.append(row)\n","        print(f\" -> {row['symbol']}: CV Sharpe {row['cv_sharpe']:.3f}, Hold Sharpe {row['hold_sharpe']:.3f}\")\n","    except Exception as e:\n","        print(f\" !! Skipped {fpath} due to error: {e}\")\n","\n","df_summary = pd.DataFrame(summary).sort_values(by=[\"hold_sharpe\",\"cv_sharpe\"], ascending=False)\n","print(\"\\n=== Multi-Ticker Summary (sorted by Hold Sharpe, then CV Sharpe) ===\")\n","print(df_summary.to_string(index=False))\n","\n","out_csv = os.path.join(DATA_DIR, \"multi_ticker_summary.csv\")\n","df_summary.to_csv(out_csv, index=False)\n","print(f\"\\nSaved summary to: {out_csv}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRnSaH0e9ow1","executionInfo":{"status":"ok","timestamp":1760128619810,"user_tz":-180,"elapsed":409053,"user":{"displayName":"Mohamed Ehab","userId":"16063572679742729614"}},"outputId":"0a1e7594-ffb8-4cbf-88ac-cebcae5677cf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[1/10] Running: /content/AAPL_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> AAPL: CV Sharpe 0.117, Hold Sharpe 0.509\n","\n","[2/10] Running: /content/AMD_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> AMD: CV Sharpe 0.164, Hold Sharpe -0.136\n","\n","[3/10] Running: /content/CVX_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> CVX: CV Sharpe 0.507, Hold Sharpe 0.686\n","\n","[4/10] Running: /content/GS_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> GS: CV Sharpe 0.563, Hold Sharpe 1.041\n","\n","[5/10] Running: /content/JPM_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> JPM: CV Sharpe 0.726, Hold Sharpe 0.748\n","\n","[6/10] Running: /content/KO_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> KO: CV Sharpe -0.014, Hold Sharpe -1.282\n","\n","[7/10] Running: /content/MSFT_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> MSFT: CV Sharpe 0.954, Hold Sharpe 0.540\n","\n","[8/10] Running: /content/NVDA_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> NVDA: CV Sharpe 0.408, Hold Sharpe -0.041\n","\n","[9/10] Running: /content/PG_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> PG: CV Sharpe 0.249, Hold Sharpe 0.871\n","\n","[10/10] Running: /content/XOM_hourly_last5y_selected_features.csv\n","[SIM] Applying Friday scaling: day×=0.75, late×=0.5, block_new_after_late=True\n"," -> XOM: CV Sharpe 1.409, Hold Sharpe -2.116\n","\n","=== Multi-Ticker Summary (sorted by Hold Sharpe, then CV Sharpe) ===\n","symbol  n_blocks  cv_sharpe  cv_turnover  cv_total_return_pct  cv_cagr_pct  cv_max_dd_pct  hold_sharpe  hold_turnover  hold_total_return_pct  hold_cagr_pct  hold_max_dd_pct  band_R  ema_half_life  sign\n","    GS      7483   0.563243     0.828678            12.231785     2.359066      -5.258322     1.040869       0.993320               5.101523       5.153936        -2.904749     2.0              4    -1\n","    PG      7479   0.248746     0.831662             6.497883     1.280869     -10.078113     0.870778       0.997326               4.243669       4.290016        -6.611311     0.8             12    -1\n","   JPM      7484   0.726033     0.831908            31.515983     5.690841     -12.468072     0.748491       0.997996               5.033984       5.085686        -5.769054     0.8             12    -1\n","   CVX      7484   0.507288     0.832576            13.773681     2.641309      -7.718467     0.685590       0.997996               2.183785       2.205907        -2.830699     1.7              6    -1\n","  MSFT      7484   0.953705     0.831507            28.446535     5.187781      -6.885153     0.540032       1.000000               3.265752       3.299009        -5.184980     0.8             12    -1\n","  AAPL      7484   0.116906     0.832843             2.771818     0.553901     -12.396684     0.509473       0.997996               2.723344       2.751004        -4.188887     0.8             12    -1\n","  NVDA      7484   0.408484     0.832175            22.437543     4.174512     -20.612370    -0.040644       0.999332              -0.997095      -1.007035        -9.914381     1.3              6    -1\n","   AMD      7484   0.163949     0.831908             5.459485     1.079717     -16.484310    -0.136473       0.998664              -1.229589      -1.241832        -8.242487     0.9              6    -1\n","    KO      7484  -0.014117     0.831641            -0.831558    -0.168561     -13.807911    -1.282056       0.998664              -8.183067      -8.261578        -9.570983     0.8             12    -1\n","   XOM      7484   1.409156     0.830973            56.798686     9.512886      -7.880554    -2.116478       0.997996              -7.157816      -7.226881        -7.997852     1.5              4    -1\n","\n","Saved summary to: /content/multi_ticker_summary.csv\n"]}]},{"cell_type":"code","source":["# ===========================\n","# FINALIZE BEST MODEL (train-on-full and save assets)\n","# Run this AFTER the multi-ticker summary cell\n","# ===========================\n","import json, math, os\n","import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","\n","assert 'df_summary' in globals() and not df_summary.empty, \"Run the multi-ticker cell first.\"\n","\n","# 1) Choose the best row by hold_sharpe, then cv_sharpe as tie-breaker\n","df_sorted = df_summary.sort_values(by=[\"hold_sharpe\",\"cv_sharpe\"], ascending=False)\n","best = df_sorted.iloc[0].to_dict()\n","best_sym = best[\"symbol\"]\n","print(f\"[BEST] {best_sym}  hold_sharpe={best['hold_sharpe']:.3f}  cv_sharpe={best['cv_sharpe']:.3f}\")\n","\n","# 2) Locate source CSV for that symbol\n","DATA_DIR = \"/content\"  # same as in the training cell\n","best_csv = os.path.join(DATA_DIR, f\"{best_sym}_hourly_last5y_selected_features.csv\")\n","if not os.path.exists(best_csv):\n","    # fallback: search by glob in case naming differs\n","    import glob, re\n","    matches = [p for p in glob.glob(os.path.join(DATA_DIR, \"*_hourly_last5y_selected_features.csv\"))\n","               if re.search(fr\"/{best_sym}_hourly_last5y_selected_features\\.csv$\", p)]\n","    if not matches:\n","        raise FileNotFoundError(f\"Could not find CSV for best symbol {best_sym} in {DATA_DIR}\")\n","    best_csv = matches[0]\n","\n","print(f\"[LOAD] {best_csv}\")\n","df_base = pd.read_csv(best_csv)\n","\n","# 3) Rebuild features for best symbol\n","(df_feat, feat_cols,\n"," X_dict, y_reg_norm_dict,\n"," sigma20_pct, hourly_ret_pct_all) = build_features_with_volnorm(df_base, HORIZON_LIST)\n","\n","# For live usage we train a single model on PRIMARY_H horizon (normalized target),\n","# and at inference we will de-normalize by sigma20 * sqrt(PRIMARY_H) (your runtime already does this).\n","X = X_dict[PRIMARY_H]\n","y_norm = y_reg_norm_dict[PRIMARY_H]  # target: y_reg_{H}h_pct / sigma20\n","\n","# 4) Train final Booster on ALL available rows (no CV here)\n","dtrain = xgb.DMatrix(X, label=y_norm)\n","final_params = dict(xgb_reg_params)  # reuse your tuned params\n","# You can reuse NUM_BOOST_ROUND; no early stopping since no eval set\n","bst = xgb.train(params=final_params, dtrain=dtrain, num_boost_round=NUM_BOOST_ROUND)\n","\n","# 5) Save artifacts for Render app\n","MODEL_PATH = \"XGboost_model.json\"\n","FEAT_PATH  = \"feat_cols.json\"\n","POLICY_PATH= \"policy.json\"\n","\n","bst.save_model(MODEL_PATH)\n","with open(FEAT_PATH, \"w\") as f:\n","    json.dump(feat_cols, f)\n","\n","policy = {\n","    \"ticker\": best_sym,\n","    \"primary_h\": int(PRIMARY_H),\n","    \"band_R\": float(best[\"band_R\"]),\n","    \"ema_half_life\": int(best[\"ema_half_life\"]),\n","    \"sign\": int(best[\"sign\"]),\n","    # runtime de-normalization convention (match your app):\n","    # pred_pct = sign * (model_pred_norm * sigma20_last * sqrt(primary_h))\n","    \"denorm\": {\"use_sigma20\": True, \"sqrt_h\": True}\n","}\n","with open(POLICY_PATH, \"w\") as f:\n","    json.dump(policy, f, indent=2)\n","\n","print(f\"[SAVED] model  → {MODEL_PATH}\")\n","print(f\"[SAVED] feats  → {FEAT_PATH}  (n={len(feat_cols)})\")\n","print(f\"[SAVED] policy → {POLICY_PATH}  {policy}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LySwatSjqsNQ","executionInfo":{"status":"ok","timestamp":1760130519517,"user_tz":-180,"elapsed":3683,"user":{"displayName":"Mohamed Ehab","userId":"16063572679742729614"}},"outputId":"0bcaa411-dd4d-479d-a9a7-810ebbbe564b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[BEST] GS  hold_sharpe=1.041  cv_sharpe=0.563\n","[LOAD] /content/GS_hourly_last5y_selected_features.csv\n","[SAVED] model  → XGboost_model.json\n","[SAVED] feats  → feat_cols.json  (n=22)\n","[SAVED] policy → policy.json  {'ticker': 'GS', 'primary_h': 1, 'band_R': 2.0, 'ema_half_life': 4, 'sign': -1, 'denorm': {'use_sigma20': True, 'sqrt_h': True}}\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
