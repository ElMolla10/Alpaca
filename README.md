Agentic E-Trading Engine

A modular, adaptive trading engine built on Reinforcement Learning (RL), contextual decision logic, and predictive modeling.
The system learns from real trading performance, adjusts behavior based on reward feedback, and executes trades with strict safety and risk controls.

This repository powers an agent that:

Adapts position sizing and timing automatically

Learns from PnL-based rewards

Uses XGBoost predictions as structured initial signals

Models slippage, commissions, and execution uncertainty

Supports user-specific trading styles

ğŸ“ Project Structure
app/
 â”œâ”€â”€ agent/
 â”‚    â”œâ”€â”€ __init__.py
 â”‚    â””â”€â”€ arl_agent.py          # Core RL logic, state updates, and policy methods
 â”‚
 â”œâ”€â”€ model/
 â”‚    â”œâ”€â”€ __init__.py
 â”‚    â””â”€â”€ XGboost_model.json    # Predictive model used for directional bias
 â”‚
 â”œâ”€â”€ execution.py               # Order execution, fills, slippage, and PnL ledger
 â”œâ”€â”€ feat_cols.json             # List of engineered features used by the model
 â”œâ”€â”€ main.py                    # Main loop orchestrating data â†’ decision â†’ execution
 â”‚
 â”œâ”€â”€ render.yaml                # Cloud deployment configuration
 â”œâ”€â”€ requirements.txt           # Python dependencies
 â”œâ”€â”€ runtime.txt                # Python runtime version
 â””â”€â”€ README.md

âš™ï¸ Core Components
ğŸ”¹ 1. RL Agent (arl_agent.py)

The RL agent is the heart of the system:

Learns from net PnL% per block

Adjusts size multiplier, timing, hold duration, and directional bias

Responds dynamically to live performance and volatility

Resets internal state daily to avoid leakage

Uses the UserStyle dataclass to shape personality:

momentum-focused

mean-reversion

conservative / aggressive risk levels

The architecture allows the agent to change behavior without hardcoding strategies.

ğŸ”¹ 2. Execution Layer (execution.py)

Handles all interaction with the broker (simulated or real):

Idempotent order placement (safe retries, no duplicates)

Partial fill handling

Slippage and commission injection for realistic reward signals

Maintains a block-based ledger tracking:

realized PnL

unrealized PnL

exposures

transaction cost impact

ğŸ”¹ 3. Predictive Model (XGboost_model.json)

An XGBoost model provides directional and volatility information:

Encodes recent OHLCV patterns

Provides a soft signal that guides the RL agent

Never acts alone â€” the agent decides when to trust or ignore predictions

This hybrid approach gives the system structured signals + adaptive learning.

ğŸ“Š Key Features

Adaptive RL policy updated every trading block

Context-aware trade sizing and timing

Realistic slippage and fee modeling

Directional bias from XGBoost predictions

Daily state reset to avoid time leakage

Extensible architecture (plug in sentiment, macro data, alternative agents)

Deterministic reward loops to stabilize learning

ğŸ§  Reward Function

The agentâ€™s reward aligns directly with profitability:

ğ‘…
ğ‘’
ğ‘¤
ğ‘
ğ‘Ÿ
ğ‘‘
=
ğ‘ƒ
ğ‘›
ğ¿
âˆ’
(
ğ¹
ğ‘’
ğ‘’
ğ‘ 
+
ğ‘†
ğ‘™
ğ‘–
ğ‘
ğ‘
ğ‘
ğ‘”
ğ‘’
)
ğ¸
ğ‘¥
ğ‘
ğ‘œ
ğ‘ 
ğ‘¢
ğ‘Ÿ
ğ‘’
Reward=
Exposure
PnLâˆ’(Fees+Slippage)
	â€‹


This ensures the system optimizes for risk-adjusted real returns, not just raw price movement.

ğŸš€ Running the System
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Run the engine
python -m app.main

3ï¸âƒ£ (Optional) Fix path issues

Make sure you run the command from the project root so imports resolve correctly.

ğŸ§© Configuration (UserStyle)

Customize trading behavior in arl_agent.py:

Parameter	Description	Example
max_symbols	Maximum number of symbols to trade	8
prefer_momentum	Bias toward trend continuation	True
risk_level	Risk appetite	"high"
base_size_mult	Starting size multiplier	1.0
base_hold_min_blocks	Minimum holding duration	2

You can define additional styles or override defaults per session.

ğŸ›¡ï¸ QA & Safety Checks

The system validates its environment continuously:

Detects missing or stale market data

Rejects orders with inconsistent size or invalid state

Prevents duplicate executions via idempotent logic

Ensures feature strictness (no future leakage or incomplete inputs)

Logs execution success rates and exposure behavior for analysis

ğŸ‘¥ Contributors

Mohamed Ehab

Abdelrahman Tamer

Mohamed Atef

Moataz Kamal

Yahia Abdelmonaem
