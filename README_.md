# ğŸ§  Agentic Reinforcement Learning Trading System

A modular, adaptive trading engine built on **Reinforcement Learning (RL)**, contextual decision logic, and predictive modeling.  
The system learns from trading performance, adjusts behavior based on reward feedback, and executes trades with strict risk and safety constraints.

---

## ğŸ“ Project Structure

```
app/
 â”œâ”€â”€ agent/
 â”‚    â”œâ”€â”€ __init__.py
 â”‚    â””â”€â”€ arl_agent.py          # Core RL logic, state updates, and policy methods
 â”‚
 â”œâ”€â”€ model/
 â”‚    â”œâ”€â”€ __init__.py
 â”‚    â””â”€â”€ XGboost_model.json    # Predictive model used for directional bias
 â”‚
 â”œâ”€â”€ execution.py               # Order execution, fills, slippage, and PnL ledger
 â”œâ”€â”€ feat_cols.json             # List of engineered features used by the model
 â”œâ”€â”€ main.py                    # Main loop orchestrating data â†’ decision â†’ execution
 â”‚
 â”œâ”€â”€ render.yaml                # Cloud deployment configuration
 â”œâ”€â”€ requirements.txt           # Python dependencies
 â”œâ”€â”€ runtime.txt                # Python runtime version
 â””â”€â”€ README.md
```

---

## âš™ï¸ Core Components

### ğŸ”¹ 1. RL Agent (`arl_agent.py`)
The RL agent is responsible for:

- Learning from **net PnL% per block**
- Adjusting **position size, timing, and hold duration**
- Reacting to performance and volatility changes
- Resetting daily to prevent leakage
- Shaping its behavior using the `UserStyle` dataclass:
  - Momentum or mean-reversion preference  
  - Risk levels (low / medium / high)

---

### ğŸ”¹ 2. Execution Layer (`execution.py`)
Handles the trading pipeline:

- Idempotent order placement (safe retries)
- Partial-fill detection and correction
- Slippage + fee modeling
- Block-based ledger tracking:
  - Realized PnL  
  - Unrealized PnL  
  - Exposure  
  - Transaction cost impact  

---

### ğŸ”¹ 3. Predictive Model (`model/XGboost_model.json`)
Provides directional bias for the RL agent:

- Encodes OHLCV patterns, volatility, and trend signals  
- Serves as a **non-deterministic hint**, not a hard decision  
- Supports hybrid learning (model signal + RL adjustment)

---

## ğŸ“Š Key Features

- Adaptive RL policy updated every trading block  
- Context-aware sizing and entry timing  
- Realistic execution via slippage + fee modeling  
- Daily state resets  
- Deterministic reward loops  
- Easy integration with other agents (sentiment, macro, LSTM, etc.)

---

## ğŸ§  Reward Function

\[
Reward = \frac{PnL - (Fees + Slippage)}{Exposure}
\]

Rewards are directly tied to profitability and risk efficiency.

---

## ğŸš€ Running the System

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the engine
```bash
python -m app.main
```

Ensure you're running from the project root so module imports resolve correctly.

---

## ğŸ§© Configuration (UserStyle)

| Parameter | Description | Example |
|----------|-------------|---------|
| `max_symbols` | Max symbols to trade | 8 |
| `prefer_momentum` | Trend-following bias | True |
| `risk_level` | User risk mode | "high" |
| `base_size_mult` | Base sizing | 1.0 |
| `base_hold_min_blocks` | Minimum holding time | 2 |

---

## ğŸ›¡ï¸ QA & Safety

- Detects missing or stale data  
- Ensures feature completeness (no leakage)  
- Prevents accidental duplicate orders  
- Tracks success rate and exposure consistency  
- Validates environment before each execution loop  

---

## ğŸ‘¥ Contributors

- Mohamed Ehab  
- Abdelrahman Tamer  
- Mohamed Atef  
- Moataz Kamal  
- Yahia Abdelmonaem  
